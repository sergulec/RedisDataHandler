import pandas as pd
import time
import re
import os
import subprocess
from datetime import datetime, timedelta
import logging
from threading import Thread

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("log_monitor.log"), logging.StreamHandler()],
)

class LogMonitor:
    def __init__(self, log_file, data_dir, output_dir, thresholds, ssh_threshold, save_interval=60, n_days=3):
        self.log_file = log_file
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.thresholds = thresholds  # Fixed thresholds per metric
        self.ssh_threshold = ssh_threshold  # Threshold for SSH count
        self.save_interval = save_interval
        self.n_days = n_days  # Number of days to consider for historical calculations
        self.metrics_data = pd.DataFrame(columns=["Metric", "Date", "Count", "Min", "Max", "Total", "Average"])
        self.ssh_data = pd.DataFrame(columns=["Date", "Time", "User", "Count"])

    def load_historical_data(self):
        """Load historical metrics data from the past n days."""
        if not os.path.exists(self.data_dir):
            logging.info(f"No historical data directory found: {self.data_dir}. Starting fresh.")
            return

        cutoff_date = datetime.now() - timedelta(days=self.n_days)
        files_loaded = False
        for file_name in os.listdir(self.data_dir):
            if file_name.endswith(".csv"):
                file_path = os.path.join(self.data_dir, file_name)
                try:
                    df = pd.read_csv(file_path, parse_dates=["Date"])
                    df = df[df["Date"] >= cutoff_date]
                    self.metrics_data = pd.concat([self.metrics_data, df], ignore_index=True)
                    files_loaded = True
                except Exception as e:
                    logging.error(f"Error loading file {file_path}: {e}")

        if not files_loaded:
            logging.info(f"No historical data found for the past {self.n_days} days. Starting fresh.")
        else:
            logging.info(f"Loaded historical data from the past {self.n_days} days.")

    def save_metrics(self):
        """Save current metrics data to a CSV file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(self.output_dir, f"metrics_{timestamp}.csv")
        try:
            self.metrics_data.to_csv(output_file, index=False)
            logging.info(f"Metrics data saved to {output_file}")
        except Exception as e:
            logging.error(f"Error saving metrics data: {e}")

    def save_ssh_data(self):
        """Save SSH count data to a CSV file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(self.output_dir, f"ssh_data_{timestamp}.csv")
        try:
            self.ssh_data.to_csv(output_file, index=False)
            logging.info(f"SSH data saved to {output_file}")
        except Exception as e:
            logging.error(f"Error saving SSH data: {e}")

    def calculate_alert_threshold(self, metric):
        """Calculate the alert threshold based on historical data."""
        historical_data = self.metrics_data[self.metrics_data["Metric"] == metric]
        if historical_data.empty:
            logging.warning(f"No historical data available for {metric}. Skipping deviation-based threshold.")
            return None  # No historical data for this metric
        mean = historical_data["Average"].mean()
        stddev = historical_data["Average"].std()
        return mean + 2 * stddev

    def update_metrics(self, metric, value):
        """Update metrics for a given metric name."""
        now = datetime.now()
        date = now.strftime("%Y-%m-%d")
        existing = self.metrics_data[self.metrics_data["Metric"] == metric]
        if not existing.empty:
            idx = existing.index[0]
            row = self.metrics_data.loc[idx]
            row["Count"] += 1
            row["Min"] = min(row["Min"], value)
            row["Max"] = max(row["Max"], value)
            row["Total"] += value
            row["Average"] = row["Total"] / row["Count"]
            self.metrics_data.loc[idx] = row
        else:
            new_row = {
                "Metric": metric,
                "Date": date,
                "Count": 1,
                "Min": value,
                "Max": value,
                "Total": value,
                "Average": value,
            }
            self.metrics_data = pd.concat([self.metrics_data, pd.DataFrame([new_row])], ignore_index=True)

        # Check for alerts
        self.check_alerts(metric, value)

    def check_alerts(self, metric, value):
        """Check and trigger alerts based on fixed and deviation thresholds."""
        # Fixed threshold alert
        if metric in self.thresholds and value > self.thresholds[metric]:
            logging.warning(f"ALERT: {metric} exceeded fixed threshold with value {value:.3f}ms "
                            f"(Threshold: {self.thresholds[metric]:.3f}ms)")

        # Deviation-based alert
        deviation_threshold = self.calculate_alert_threshold(metric)
        if deviation_threshold is not None and value > deviation_threshold:
            logging.warning(f"ALERT: {metric} exceeded deviation-based threshold with value {value:.3f}ms "
                            f"(Threshold: {deviation_threshold:.3f}ms)")

    def monitor_log(self):
        """Monitor log file for latency metrics."""
        last_save_time = time.time()
        with open(self.log_file, "r") as log_file:
            log_file.seek(0, 2)  # Move to the end of the file
            while True:
                line = log_file.readline()
                if not line:
                    time.sleep(0.1)  # Wait for new log entries
                    continue

                # Match the log line with the pattern
                match = re.search(r"LATENCY:(?P<metric>\w+):(?P<value>[\d.]+)ms", line)
                if match:
                    metric = match.group("metric")
                    value = float(match.group("value"))
                    self.update_metrics(metric, value)

                # Save metrics data and log DataFrames at regular intervals
                current_time = time.time()
                if current_time - last_save_time >= self.save_interval:
                    self.save_metrics()
                    self.save_ssh_data()
                    logging.info(f"Current metrics DataFrame:\n{self.metrics_data}")
                    logging.info(f"Current SSH DataFrame:\n{self.ssh_data}")
                    last_save_time = current_time

    def run_ssh_command(self, interval=60):
        """Run the SSH count command at regular intervals."""
        while True:
            now = datetime.now()
            date, time_str = now.strftime("%Y-%m-%d"), now.strftime("%H:%M:%S")
            try:
                # Run the SSH count command
                command = "grep 'exe=.*ssh' /var/log/messages | cut -d ' ' -f 15 | sort -n -k 1,2 | uniq -c"
                output = subprocess.check_output(command, shell=True, text=True)

                # Parse command output
                for line in output.splitlines():
                    parts = line.split()
                    if len(parts) >= 2:
                        count, user = int(parts[0]), parts[1].split("=")[-1]
                        new_row = {"Date": date, "Time": time_str, "User": user, "Count": count}
                        self.ssh_data = pd.concat([self.ssh_data, pd.DataFrame([new_row])], ignore_index=True)

                        # Check SSH count threshold
                        if count > self.ssh_threshold:
                            logging.warning(f"ALERT: SSH count for user {user} exceeded threshold with count {count} "
                                            f"(Threshold: {self.ssh_threshold})")
            except subprocess.CalledProcessError as e:
                logging.error(f"Error running SSH command: {e}")

            # Sleep until the next interval
            time.sleep(interval)


if __name__ == "__main__":
    log_file_path = "latency.log"  # Replace with your log file path
    data_directory = "historical_data"  # Directory where historical data is stored
    output_directory = "metrics_output"  # Directory to save current metrics data
    thresholds = {
        "trans_log_append": 0.01,  # Fixed threshold for this metric
    }
    ssh_threshold = 10  # Fixed threshold for SSH count

    # Create directories if they don't exist
    os.makedirs(data_directory, exist_ok=True)
    os.makedirs(output_directory, exist_ok=True)

    monitor = LogMonitor(
        log_file=log_file_path,
        data_dir=data_directory,
        output_dir=output_directory,
        thresholds=thresholds,
        ssh_threshold=ssh_threshold,
    )

    try:
        # Load historical data
        monitor.load_historical_data()

        # Start monitoring the log file and SSH counts in parallel
        log_thread = Thread(target=monitor.monitor_log)
        ssh_thread = Thread(target=monitor.run_ssh_command)
        log_thread.start()
        ssh_thread.start()
        log_thread.join()
        ssh_thread.join()
    except KeyboardInterrupt:
        logging.info("Monitoring stopped.")
        monitor.save_metrics()
        monitor.save_ssh_data()
